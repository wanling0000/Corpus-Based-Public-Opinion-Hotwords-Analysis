{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二、对微博，知乎，央视三个语料库进行jieba分词，词频统计，词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1、微博"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "import os\n",
    "import jieba.analyse\n",
    "file=open(\"commentOfWeibo.txt\",\"r\",encoding='utf-8')\n",
    "wbcomment=file.read()\n",
    "file.close\n",
    "newwbcomment=re.sub(\"[^\\u4e00-\\u9fa5]\", \"\",wbcomment) #删除原文中的数字，标点符号和空格，只留下中文字符\n",
    "print(newwbcomment[:500])\n",
    "\n",
    "#分词\n",
    "outputfolder = \"weibo.tokenized\"\n",
    "cut_result=' '.join(jieba.cut(newwbcomment))\n",
    "outputfilename = 'weibocut.txt'\n",
    "output_file=open(outputfilename,\"w\",encoding=\"utf-8\")\n",
    "output_file.write(cut_result)\n",
    "output_file.flush()\n",
    "output_file.close()\n",
    "\n",
    "#统计词频\n",
    "word_total=0 #总词数\n",
    "word_count={}#词的出现数\n",
    "word_freq=[]\n",
    "\n",
    "words=cut_result.split()\n",
    "for char in words:    \n",
    "    if char not in word_count:\n",
    "        word_count[char]=1\n",
    "    else:\n",
    "        word_count[char]=word_count[char]+1\n",
    "    word_total=word_total+1\n",
    "sorted_word_count=sorted(word_count,key=word_count.get,reverse=True)        \n",
    "for char in sorted_word_count:\n",
    "    word_freq.append([char,word_count[char],word_count[char]/len(words)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"weibo.xls\")\n",
    "\n",
    "#词云\n",
    "df = pd.read_excel('C:/Users/fwl/Desktop/weibo.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"freq\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"weibo.jpg\") # 将图片输出为文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2、知乎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba.analyse\n",
    "inputfolder = \"zhihu\" #输入文件夹，里有待分词的文件\n",
    "outputfolder = \"zhihu.tokenized\" #输出文件夹，存放分好词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "res=\"\"\n",
    "i=1\n",
    "for filename in inputfiles:\n",
    "    \n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +\"zhihu\" + str(i) # 分好词的文件名\n",
    "    i=i+1\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    \n",
    "    res+=in_data\n",
    "    cut_result=' '.join(jieba.cut(in_data))\n",
    "    print(cut_result)\n",
    "    #将分词结果cut_result写入文件output_file\n",
    "    output_file=open(outputfilename,\"w\",encoding=\"utf-8\")\n",
    "    output_file.write(cut_result)\n",
    "    output_file.flush()\n",
    "    output_file.close()\n",
    "\n",
    "folder = \"zhihu.tokenized\" # 从分词后的文件夹中读取文件\n",
    "files = os.listdir(folder)\n",
    "word_total=0 #总词数\n",
    "word_count={}#词的出现数\n",
    "word_freq=[]\n",
    "\n",
    "for file in files:\n",
    "    filename = folder + \"/\" + file\n",
    "    current_file=open(filename, \"r\", encoding=\"utf-8\")\n",
    "    data_from_file=current_file.read()\n",
    "    current_file.close\n",
    "    data_from_file=re.sub(r\"[^\\s\\u4e00-\\u9fa5]\", \"\", data_from_file)#去掉标点符号，数字等，只保留汉字和空格\n",
    "    words=data_from_file.split()\n",
    "    #进行词频统计。并将词频统计的结果存入EXCEL文件\n",
    "    for char in words:    \n",
    "        if char not in word_count:\n",
    "            word_count[char]=1\n",
    "        else:\n",
    "            word_count[char]=word_count[char]+1\n",
    "        word_total=word_total+1\n",
    "sorted_word_count=sorted(word_count,key=word_count.get,reverse=True)        \n",
    "for char in sorted_word_count:\n",
    "    word_freq.append([char,word_count[char],word_count[char]/len(words)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"zhihu.xlsx\")\n",
    "\n",
    "print(word_freq)\n",
    "\n",
    "#词云制作\n",
    "df = pd.read_excel('zhihutf_idf.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"freq\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"zhihuciping.jpg\") # 将图片输出为文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3、央视"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba.analyse\n",
    "inputfolder = \"yangshi\" #输入文件夹，里有待分词的文件\n",
    "outputfolder = \"yangshi.tokenized\" #输出文件夹，存放分好词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "res=\"\"\n",
    "i=1\n",
    "for filename in inputfiles:\n",
    "    \n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +\"yangshi\" + str(i) # 分好词的文件名\n",
    "    i=i+1\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    #请在此处补充代码，完成分词，词与词之间用空格隔开，存入cut_result中 \n",
    "    res+=in_data\n",
    "    cut_result=' '.join(jieba.cut(in_data))\n",
    "    print(cut_result)\n",
    "    #将分词结果cut_result写入文件output_file\n",
    "    output_file=open(outputfilename,\"w\",encoding=\"utf-8\")\n",
    "    output_file.write(cut_result)\n",
    "    output_file.flush()\n",
    "    output_file.close()\n",
    "\n",
    "folder = \"yangshi.tokenized\" # 从分词后的文件夹中读取文件\n",
    "files = os.listdir(folder)\n",
    "word_total=0 #总词数\n",
    "word_count={}#词的出现数\n",
    "word_freq=[]\n",
    "\n",
    "for file in files:\n",
    "    filename = folder + \"/\" + file\n",
    "    current_file=open(filename, \"r\", encoding=\"utf-8\")\n",
    "    data_from_file=current_file.read()\n",
    "    current_file.close\n",
    "    data_from_file=re.sub(r\"[^\\s\\u4e00-\\u9fa5]\", \"\", data_from_file)#去掉标点符号，数字等，只保留汉字和空格\n",
    "    words=data_from_file.split()\n",
    "    for char in words:    \n",
    "        if char not in word_count:\n",
    "            word_count[char]=1\n",
    "        else:\n",
    "            word_count[char]=word_count[char]+1\n",
    "        word_total=word_total+1\n",
    "sorted_word_count=sorted(word_count,key=word_count.get,reverse=True)        \n",
    "for char in sorted_word_count:\n",
    "    word_freq.append([char,word_count[char],word_count[char]/len(words)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"yangshi.xlsx\")\n",
    "\n",
    "print(word_freq)\n",
    "\n",
    "#词云制作\n",
    "df = pd.read_excel('yangshi_idf.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"freq\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"yangshiciping.jpg\") # 将图片输出为文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 四、对微博，央视，知乎进行词性标注及词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1、微博"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#人名\n",
    "import jieba.posseg as psg \n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "import os\n",
    "import jieba.analyse\n",
    "\n",
    "result = []\n",
    "location = []\n",
    "person = []\n",
    "\n",
    "file=open(\"commentOfWeibo.txt\",\"r\",encoding='utf-8')\n",
    "wbcomment=file.read()\n",
    "cut_result = psg.cut(wbcomment)\n",
    "\n",
    "for m in cut_result:\n",
    "    if len(m.word) == 1:\n",
    "        continue\n",
    "    if m.flag == 'nr':\n",
    "        person.append(m.word)\n",
    "\n",
    "for j in result:\n",
    "    if len(j[0])==1:\n",
    "        continue\n",
    "    if j[1] == 'ns':\n",
    "        location.append(j[0])\n",
    "\n",
    "#人名词频\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"wb人名.xlsx\") \n",
    "\n",
    "#地名\n",
    "import jieba.posseg as psg \n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "import os\n",
    "import jieba.analyse\n",
    "\n",
    "result = []\n",
    "location = []\n",
    "person = []\n",
    "\n",
    "file=open(\"commentOfWeibo.txt\",\"r\",encoding='utf-8')\n",
    "wbcomment=file.read()\n",
    "cut_result = psg.cut(wbcomment)\n",
    "\n",
    "for m in cut_result:\n",
    "    if len(m.word) == 1:\n",
    "        continue\n",
    "    if m.flag == 'ns':\n",
    "        person.append(m.word)\n",
    "\n",
    "\n",
    "\n",
    "#地名词频\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"wb地名.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2、知乎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#人名\n",
    "import jieba.posseg as psg \n",
    "import os\n",
    "import pandas as pd\n",
    "person = []\n",
    "result = []\n",
    "inputfolder = \"zhihu\" #输入文件夹，里有待分词的文件\n",
    "outputfolder = \"zhihufenci\" #输出文件夹，存放分好词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "for filename in inputfiles:\n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +filename # 分好词的文件名\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    #请在此处补充代码，完成分词，词与词之间用空格隔开，存入cut_result中 \n",
    "    cut_result=psg.cut(in_data)\n",
    "    # for i in cut_result:\n",
    "    #     result.append([i.word,i.flag])\n",
    "    for m in cut_result:\n",
    "        if len(m.word) == 1:\n",
    "            continue\n",
    "        if m.flag == 'nr':\n",
    "            person.append(m.word)\n",
    "\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"知乎人名.xlsx\")  \n",
    "\n",
    "#地名\n",
    "import jieba.posseg as psg \n",
    "import os\n",
    "import pandas as pd\n",
    "person = []\n",
    "result = []\n",
    "inputfolder = \"zhihu\" #输入文件夹，里有待分词的文件\n",
    "outputfolder = \"zhihufenci\" #输出文件夹，存放分好词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "for filename in inputfiles:\n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +filename # 分好词的文件名\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    #请在此处补充代码，完成分词，词与词之间用空格隔开，存入cut_result中 \n",
    "    cut_result=psg.cut(in_data)\n",
    "    # for i in cut_result:\n",
    "    #     result.append([i.word,i.flag])\n",
    "    for m in cut_result:\n",
    "        if len(m.word) == 1:\n",
    "            continue\n",
    "        if m.flag == 'ns':\n",
    "            person.append(m.word)\n",
    "\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"知乎地名.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3、央视"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#人名\n",
    "import jieba.posseg as psg \n",
    "import os\n",
    "import pandas as pd\n",
    "person = []\n",
    "result = []\n",
    "inputfolder = \"yangshi\" #输入文件夹，里有待分词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "for filename in inputfiles:\n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +filename # 分好词的文件名\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    #请在此处补充代码，完成分词，词与词之间用空格隔开，存入cut_result中 \n",
    "    cut_result=psg.cut(in_data)\n",
    "    # for i in cut_result:\n",
    "    #     result.append([i.word,i.flag])\n",
    "    for m in cut_result:\n",
    "        if len(m.word) == 1:\n",
    "            continue\n",
    "        if m.flag == 'nr':\n",
    "            person.append(m.word)\n",
    "\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"央视人名.xlsx\")  \n",
    "\n",
    "#地名\n",
    "import jieba.posseg as psg \n",
    "import os\n",
    "import pandas as pd\n",
    "person = []\n",
    "result = []\n",
    "inputfolder = \"yangshi\" #输入文件夹，里有待分词的文件\n",
    "inputfiles = os.listdir(inputfolder) #获取输入文件夹中的所有文件名\n",
    "for filename in inputfiles:\n",
    "    inputfilename = inputfolder + \"/\" + filename # 待分词的文件名\n",
    "    outputfilename = outputfolder + \"/\" +filename # 分好词的文件名\n",
    "    input_file=open(inputfilename, \"r\", encoding=\"utf-8\")\n",
    "    in_data=input_file.read() #读入input_file的文件内容到字符串in_data\n",
    "    input_file.close       \n",
    "    #请在此处补充代码，完成分词，词与词之间用空格隔开，存入cut_result中 \n",
    "    cut_result=psg.cut(in_data)\n",
    "    # for i in cut_result:\n",
    "    #     result.append([i.word,i.flag])\n",
    "    for m in cut_result:\n",
    "        if len(m.word) == 1:\n",
    "            continue\n",
    "        if m.flag == 'ns':\n",
    "            person.append(m.word)\n",
    "\n",
    "person_count = {}\n",
    "person_total = 0\n",
    "word_freq=[]\n",
    "for char in person:\n",
    "    if char not in person_count:\n",
    "        person_count[char]=1\n",
    "    else:\n",
    "        person_count[char]=person_count[char]+1\n",
    "        person_total=person_total+1\n",
    "sorted_person_count=sorted(person_count,key=person_count.get,reverse=True)\n",
    "\n",
    "for char in sorted_person_count:\n",
    "    word_freq.append([char,person_count[char],person_count[char]/len(person)])   \n",
    "    \n",
    "word_freq=pd.DataFrame(word_freq, columns=[\"char\", \"count\", \"freq\"]).set_index(\"char\")   #set表示该列作为索引列\n",
    "word_freq.to_excel(\"央视地名.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4、词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微博\n",
    "#国内地区（县及以下）词云\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location国内地区（县及以下）.jpg\") # 将图片输出为文件\n",
    "\n",
    "#国内地区（省级）词云\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location国内地区（省级）.jpg\") # 将图片输出为文件\n",
    "\n",
    "#国际关系词云\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location国际关系.jpg\") # 将图片输出为文件\n",
    "\n",
    "#其他国家和地区词云\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location其他国家和地区.jpg\") # 将图片输出为文件\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location国内地理分区.jpg\") # 将图片输出为文件\n",
    "\n",
    "# 国内行政区\n",
    "import pandas as pd\n",
    "df = pd.read_excel('wb_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"wb_location国内行政区.jpg\") # 将图片输出为文件\n",
    "\n",
    "#知乎\n",
    "#国内地理区\n",
    "import pandas as pd\n",
    "df = pd.read_excel('zh_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"zh_location国内地区（县及以下）.jpg\") # 将图片输出为文件\n",
    "\n",
    "#国内行政区\n",
    "import pandas as pd\n",
    "df = pd.read_excel('zh_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"zh_location国内行政区.jpg\") # 将图片输出为文件\n",
    "\n",
    "#非典型地区等\n",
    "import pandas as pd\n",
    "df = pd.read_excel('zh_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"zh_location非典型地区等.jpg\") # 将图片输出为文件\n",
    "\n",
    "#其他国家和地区\n",
    "import pandas as pd\n",
    "df = pd.read_excel('zh_location词频.xlsx') #从词频文件中读取数据到新的词典word_freqs\n",
    "word_freqs={}\n",
    "for i in df.index.values:  #获取行号的索引，并对其进行遍历\n",
    "    word_freqs[df.loc[i,'char']]=df.loc[i,\"count\"]\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white', # 设置背景颜色\n",
    "    font_path='C:\\Windows\\Fonts\\simkai.ttf', # 设置字体格式\n",
    "    max_words=500, # 最多显示词数\n",
    "    max_font_size=100 , # 字体最大值\n",
    "    scale=64,  # 调整图片清晰度，值越大越清楚    \n",
    "  )\n",
    "word_cloud.generate_from_frequencies(word_freqs) #利用词典word_freqs生成词云\n",
    "word_cloud.to_file(\"zh_location其他国家和地区.jpg\") # 将图片输出为文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
